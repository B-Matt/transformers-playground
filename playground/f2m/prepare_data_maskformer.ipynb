{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/matej/B4085A48085A09AE/Posao/Transformers-Playground/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import torch\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import MaskFormerForInstanceSegmentation, MaskFormerImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = { 0: 'background', 1: 'fire' }\n",
    "label2id = { 'background': 0, 'fire': 1 }\n",
    "\n",
    "models_data = [\n",
    "    {\n",
    "        'model_name': 'MaskFormer 256x256',\n",
    "        'model_path': r'/media/matej/B4085A48085A09AE/Posao/Transformers-Playground/checkpoints/maskformer/800x800-2/checkpoint-248576/',\n",
    "        'patch_size': 800,\n",
    "    },\n",
    "]\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize_and_pad(image: np.array, \n",
    "                    new_shape,\n",
    "                    padding_color = (0, 0, 0)\n",
    "                ) -> np.array:\n",
    "        \"\"\"\n",
    "        Maintains aspect ratio and resizes with padding.\n",
    "        Params:\n",
    "            image: Image to be resized.\n",
    "            new_shape: Expected (width, height) of new image.\n",
    "            padding_color: Tuple in BGR of padding color\n",
    "        Returns:\n",
    "            image: Resized image with padding\n",
    "        \"\"\"\n",
    "        original_shape = (image.shape[1], image.shape[0])\n",
    "        ratio = float(max(new_shape)) / max(original_shape)\n",
    "        new_size = tuple([int(x*ratio) for x in original_shape])\n",
    "        image = cv2.resize(image, new_size)\n",
    " \n",
    "        delta_w = new_shape[0] - new_size[0]\n",
    "        delta_h = new_shape[1] - new_size[1]\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    " \n",
    "        image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=padding_color)\n",
    "        return image\n",
    "\n",
    "def preload_image_data(data_dir, img_dir, is_mask: bool = False, patch_size: int = 256, dataset_list = 'test_dataset.txt'):\n",
    "    \"\"\"\n",
    "        Loads all images from data_dir.\n",
    "    \"\"\"\n",
    "    dataset_files = []\n",
    "    dataset_file_names = []\n",
    "    with open(pathlib.Path('/media/matej/B4085A48085A09AE/Posao/Transformers-Playground', data_dir, dataset_list), mode='r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            path = pathlib.Path('/media/matej/B4085A48085A09AE/Posao/Transformers-Playground', data_dir, img_dir, line.strip(), f'Image/{line.strip()}.png' if is_mask == False else f'Mask/0.png')\n",
    "            img = None\n",
    "\n",
    "            # Load image\n",
    "            if is_mask:\n",
    "                img = np.array(Image.open(str(path)).convert(\"L\")) #cv2.imread(str(Path(info.mask, '0.png')), cv2.IMREAD_GRAYSCALE)\n",
    "                img = _resize_and_pad(img, (patch_size, patch_size), (0, 0, 0)).astype(np.float32)\n",
    "            else:\n",
    "                img = np.array(Image.open(str(path)).convert(\"RGB\"))  #cv2.imread(str(Path(info.image, os.listdir(info.image)[0])))\n",
    "                img = _resize_and_pad(img, (patch_size, patch_size), (0, 0, 0)).astype(np.float32)\n",
    "\n",
    "            dataset_files.append(img)\n",
    "            dataset_file_names.append(line.strip())\n",
    "\n",
    "    return dataset_files, dataset_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, _ = preload_image_data(r'dataset', r'imgs', False, models_data[0]['patch_size'], f'test_dataset.txt')\n",
    "masks, _ = preload_image_data(r'dataset', r'imgs', True, models_data[0]['patch_size'], f'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/matej/B4085A48085A09AE/Posao/Transformers-Playground/.venv/lib/python3.11/site-packages/transformers/models/maskformer/image_processing_maskformer.py:410: FutureWarning: The `size_divisibility` argument is deprecated and will be removed in v4.27. Please use `size_divisor` instead.\n",
      "  warnings.warn(\n",
      "/media/matej/B4085A48085A09AE/Posao/Transformers-Playground/.venv/lib/python3.11/site-packages/transformers/models/maskformer/image_processing_maskformer.py:417: FutureWarning: The `max_size` argument is deprecated and will be removed in v4.27. Please use size['longest_edge'] instead.\n",
      "  warnings.warn(\n",
      "Some weights of MaskFormerForInstanceSegmentation were not initialized from the model checkpoint at /media/matej/B4085A48085A09AE/Posao/Transformers-Playground/checkpoints/maskformer/800x800-2/checkpoint-248576/ and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([2, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-tiny-ade')\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\n",
    "    models_data[0]['model_path'],\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = val_transforms(image=imgs[100], mask=masks[100])\n",
    "data['image'] = data['image'].unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'pixel_mask'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = val_transforms(image=imgs[100], mask=masks[100])\n",
    "inputs = processor(data['image'], return_tensors=\"pt\")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values\"] = inputs[\"pixel_values\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_classes = outputs.class_queries_logits.softmax(dim=-1)[..., :-1]\n",
    "masks_probs = outputs.masks_queries_logits.sigmoid()\n",
    "segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_pred = torch.nn.functional.interpolate(segmentation, size=data['mask'].shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "masks_pred = masks_pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAEAAQABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiu5+HPw41XxxqKyx2u3R0dobi8ZgFibYSMDILEEqcD1GcA1xM0MttPJBPE8U0bFHjdSrKwOCCD0IPamUUUUUUUUUUUUUUUUUUUUUUUUUUUUUV1fw88ZQ+BfErazLpKak4t3iiRpfLMbMR84ba3O0MvToxrm769uNRv7m+u5DLc3MrTSyEAbnYkscDjkk1BRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRXp3gfRvDusfDHxHaXOq6FY6/c3MK2r6pMsRijQqxZWOSNwMinaO3PB48/wBZ0t9G1afT3urS7MJH7+zmEsUgIBBVh1GDVGiiiiiiiiiiiiiiiiiiiiiiiiiiiiitjw54bvfE17c29pJBClpayXlzPcMQkMKD5mOAWPUDCgnnp1rHoooooooooooooooooooooooooooooor1r4SeHbl/CXjjxIR+4TR7mwiUkYkcx72zzngBOowd/sa8uvdPvdOlSK+s7i1kdBIqTxFCyHowBHIPrVaiiiiiiiiiiiiiiiiiiiiiiiiiiiiiu58E/FbxB4D0240/TIbCe2mm88rdRM21yApIKsp5Cr1z0+tcrrmptrev6jqroY2vbmS4KFt23exbGcDOM46CqFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACEUlEQVR4Ae3asU0DMRQGYMMCkZiABhrEAIzADOyRKspSrBKkdCBRkxEwl6ShTGP9L/J39cnv3XfPPvmdW3MRIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQuEBg+3TBTUNvuR06+hUMngbYXYHRwBRXvQ8cvfzQb325ymc5MMHj8/f9wAD1h76bvATarvf0KngTLZNlBcgm0Fr6Mxj1F5wAAQIECBAgQIAAAQIRgcdI1EJBKzTEktvhTYV3kQSo8PzRhkyBflC8I9QPh3WJQogksTn1xaf+M3B0f47gC0qAAAECBQS+J/8I9h4/I5StggLbwek3Q1mArxafAlmA++wMjEd/KbAGRBE+CwBET6hU6Igk14DfpfymXgRP1bfu79FpmAx+bgg9JFPInlI77wOiq1C2Kfr6s7z8j2wB5KOnT4rmBWRAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgMB/gT/PED2yzEKaEAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(masks[100].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m masks_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmasks_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(masks_pred))\n\u001b[1;32m      3\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(masks_pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "masks_pred = masks_pred.cpu().numpy()\n",
    "print(np.unique(masks_pred))\n",
    "Image.fromarray(masks_pred[0].astype(np.uint8) * 255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
